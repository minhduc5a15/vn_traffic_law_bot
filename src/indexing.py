import os
import pickle
import torch
from dotenv import load_dotenv
from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings  # Thay Ä‘á»•i thÆ° viá»‡n import
from langchain_community.retrievers import BM25Retriever
from langchain_core.documents import Document

# Load biáº¿n mÃ´i trÆ°á»ng
load_dotenv()


class Indexer:

    def __init__(
        self,
        db_path="./data/indexes/chroma_db",
        bm25_path="./data/indexes/bm25_retriever.pkl",
    ):
        self.db_path = db_path
        self.bm25_path = bm25_path

        device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"âš™ï¸  Thiáº¿t bá»‹ cháº¡y Embedding: {device.upper()}")

        # Sá»­ dá»¥ng model tiáº¿ng Viá»‡t chuyÃªn dá»¥ng tá»« BKAI hoáº·c VinAI
        # model_name = "bkai-foundation-models/vietnamese-bi-encoder"
        model_name = "bkai-foundation-models/vietnamese-bi-encoder"

        print(f"ğŸ“¥ Äang táº£i/Load model: {model_name}...")
        self.embeddings = HuggingFaceEmbeddings(
            model_name=model_name,
            model_kwargs={"device": device},
            encode_kwargs={
                "normalize_embeddings": True
            },  # Quan trá»ng cho so sÃ¡nh cosine similarity
        )

    def build_indices(self, documents: list[Document]):
        print(f"ğŸ“Š Äang táº¡o Index cho {len(documents)} documents...")

        # 1. XÃ¢y dá»±ng Vector Store (ChromaDB)
        print("   -> ğŸ§  Äang embedding vÃ  lÆ°u vÃ o ChromaDB (Local sBERT)...")

        # XÃ³a DB cÅ© Ä‘á»ƒ clean
        if os.path.exists(self.db_path):
            import shutil

            shutil.rmtree(self.db_path)

        # Chroma tá»± Ä‘á»™ng gá»i model embedding Ä‘á»ƒ vector hÃ³a documents
        # Batch size máº·c Ä‘á»‹nh cÃ³ thá»ƒ chá»‰nh náº¿u bá»‹ OOM (Out of Memory)
        vectorstore = Chroma.from_documents(
            documents=documents,
            embedding=self.embeddings,
            persist_directory=self.db_path,
            collection_metadata={"hnsw:space": "cosine"},  # DÃ¹ng Cosine Similarity
        )
        print("   -> âœ… ÄÃ£ lÆ°u Vector Index.")

        # 2. XÃ¢y dá»±ng BM25 Retriever (Keyword Search)
        print("   -> ğŸ” Äang táº¡o chá»‰ má»¥c BM25 (Keyword Search)...")

        # Tokenizer cÆ¡ báº£n cho BM25 (tÃ¡ch tá»« theo khoáº£ng tráº¯ng lÃ  táº¡m á»•n cho BM25 á»Ÿ bÆ°á»›c nÃ y,
        # hoáº·c dÃ¹ng pyvi náº¿u muá»‘n chÃ­nh xÃ¡c hÆ¡n, nhÆ°ng máº·c Ä‘á»‹nh váº«n hoáº¡t Ä‘á»™ng tá»‘t)
        bm25_retriever = BM25Retriever.from_documents(documents)
        bm25_retriever.k = 10  # Láº¥y rá»™ng ra má»™t chÃºt cho BM25

        with open(self.bm25_path, "wb") as f:
            pickle.dump(bm25_retriever, f)
        print("   -> âœ… ÄÃ£ lÆ°u BM25 Index.")

        return vectorstore, bm25_retriever

    def load_indices(self):
        """HÃ m load láº¡i index"""
        print("ğŸ“‚ Äang táº£i láº¡i Index tá»« Ä‘Ä©a...")

        vectorstore = Chroma(
            persist_directory=self.db_path, embedding_function=self.embeddings
        )

        if os.path.exists(self.bm25_path):
            with open(self.bm25_path, "rb") as f:
                bm25_retriever = pickle.load(f)
        else:
            raise FileNotFoundError("ChÆ°a tÃ¬m tháº¥y file BM25 index.")

        return vectorstore, bm25_retriever
