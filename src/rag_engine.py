import os

os.environ["ANONYMIZED_TELEMETRY"] = "False"

from langchain_chroma import Chroma
from langchain_community.retrievers import BM25Retriever
from langchain.prompts import ChatPromptTemplate
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_huggingface import HuggingFaceEmbeddings
import pickle

from src.reranker import Reranker


class TrafficLawRAG:

    def __init__(
        self,
        vector_db_path="./data/indexes/chroma_db",
        bm25_path="./data/indexes/bm25_retriever.pkl",
    ):
        print("üöÄ Kh·ªüi ƒë·ªông Traffic Law RAG Engine (v2.0 - Query Expansion)...")

        # 1. Load Embeddings
        device = "cpu"
        self.embedding_model = HuggingFaceEmbeddings(
            model_name="bkai-foundation-models/vietnamese-bi-encoder",
            model_kwargs={"device": device},
            encode_kwargs={"normalize_embeddings": True},
        )

        # 2. Vector DB
        self.vector_db = Chroma(
            persist_directory=vector_db_path, embedding_function=self.embedding_model
        )

        # 3. BM25
        with open(bm25_path, "rb") as f:
            self.bm25_retriever = pickle.load(f)
        self.bm25_retriever.k = 15  # L·∫•y top 15 BM25

        # 4. Reranker
        self.reranker = Reranker()

        # 5. LLM Ch√≠nh & LLM Query Gen
        api_key = os.getenv("GOOGLE_API_KEY")
        self.llm = ChatGoogleGenerativeAI(
            model="gemini-2.5-flash", temperature=0, api_key=api_key
        )

        # Prompt
        self.answer_prompt = ChatPromptTemplate.from_messages(
            [
                (
                    "system",
                    """B·∫°n l√† Tr·ª£ l√Ω Lu·∫≠t Giao th√¥ng AI.
            S·ª≠ d·ª•ng th√¥ng tin sau ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi. 
            - Tr√≠ch d·∫´n ch√≠nh x√°c (Ngh·ªã ƒë·ªãnh, ƒêi·ªÅu, Kho·∫£n).
            - N·∫øu kh√¥ng c√≥ th√¥ng tin, h√£y n√≥i kh√¥ng bi·∫øt.
            
            CONTEXT:
            {context}
            """,
                ),
                ("human", "{question}"),
            ]
        )

        # Prompt bi·∫øn ƒë·ªïi c√¢u h·ªèi
        self.query_transform_prompt = ChatPromptTemplate.from_messages(
            [
                (
                    "system",
                    """B·∫°n l√† chuy√™n gia ph√°p l√Ω. Nhi·ªám v·ª• c·ªßa b·∫°n l√† vi·∫øt l·∫°i c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng th√†nh m·ªôt c√¢u truy v·∫•n t√¨m ki·∫øm chu·∫©n x√°c trong vƒÉn b·∫£n lu·∫≠t.
            - D√πng t·ª´ ng·ªØ chuy√™n ng√†nh (V√≠ d·ª•: "v∆∞·ª£t ƒë√®n ƒë·ªè" -> "kh√¥ng ch·∫•p h√†nh hi·ªáu l·ªánh c·ªßa ƒë√®n t√≠n hi·ªáu giao th√¥ng").
            - Gi·ªØ nguy√™n √Ω ƒë·ªãnh t√¨m m·ª©c ph·∫°t ho·∫∑c h√†nh vi.
            - Ch·ªâ tr·∫£ v·ªÅ c√¢u vi·∫øt l·∫°i, kh√¥ng gi·∫£i th√≠ch g√¨ th√™m.""",
                ),
                ("human", "C√¢u h·ªèi: {question}"),
            ]
        )

    def generate_legal_query(self, user_query: str):
        print(f"   üîÑ ƒêang chu·∫©n h√≥a c√¢u h·ªèi: '{user_query}'")
        response = (self.query_transform_prompt | self.llm).invoke(
            {"question": user_query}
        )
        legal_query = response.content.strip()
        print(f"   -> üéØ Query Lu·∫≠t: '{legal_query}'")
        return legal_query

    def retrieve_hybrid(self, query: str, top_k_final=5):
        search_query = self.generate_legal_query(query)

        docs_vector = self.vector_db.similarity_search(search_query, k=40)
        docs_bm25 = self.bm25_retriever.invoke(search_query)

        unique_docs = {}
        for doc in docs_vector + docs_bm25:
            key = doc.metadata.get("citation", doc.page_content[:50])
            unique_docs[key] = doc
        merged_docs = list(unique_docs.values())

        print(f"   -> T√¨m th·∫•y {len(merged_docs)} t√†i li·ªáu ti·ªÅm nƒÉng.")

        print("   -> ‚öñÔ∏è  Reranking...")
        final_docs = self.reranker.rank_documents(query, merged_docs, top_k=top_k_final)

        return final_docs

    def chat(self, user_query: str):
        context_docs = self.retrieve_hybrid(user_query)

        if not context_docs:
            return "Xin l·ªói, kh√¥ng t√¨m th·∫•y t√†i li·ªáu li√™n quan.", []

        # Format Context
        context_text = ""
        for i, doc in enumerate(context_docs):
            source = doc.metadata.get("citation", "N/A")
            content = doc.page_content.replace("\n", " ")
            context_text += f"[{i+1}] {source}: {content}\n\n"

        # Generation
        chain = self.answer_prompt | self.llm
        response = chain.invoke({"context": context_text, "question": user_query})

        return response.content, context_docs
